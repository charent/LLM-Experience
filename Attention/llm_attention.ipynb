{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Qwen2ForCausalLM\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from dataclasses import dataclass\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see: https://charent.github.io/p/llama2模型结构方面的改进/\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, hidden_size: int, eps: float=1e-6):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.weight = nn.Parameter(torch.ones(hidden_size))\n",
    "        self.eposilon = eps\n",
    "    \n",
    "    def forward(self, hidden_states: Tensor):\n",
    "        dtype = hidden_states.dtype\n",
    "        hidden_states = hidden_states.to(dtype=torch.float32)\n",
    "\n",
    "        mean_square = hidden_states.pow(2).mean(dim=-1, keepdim=True)\n",
    "\n",
    "        # 注意，这里不是sqrt， rsqrt(x) = 1 / sqrt(x)\n",
    "        hidden_states = hidden_states * torch.rsqrt(mean_square + self.eposilon)\n",
    "        hidden_states = self.weight * hidden_states\n",
    "\n",
    "        return hidden_states.to(dtype=dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_half(x: Tensor):\n",
    "    embedding_dim_half = x.shape[-1] // 2\n",
    "    x1 = x[..., : embedding_dim_half]\n",
    "    x2 = x[..., embedding_dim_half: ]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "\n",
    "\n",
    "def apply_rotay_pos_emb(q: Tensor, k: Tensor, cos: Tensor, sin: Tensor, position_ids: Tensor, unsqueeze_dim: int=1):\n",
    "    cos = cos.unsqueeze(unsqueeze_dim)\n",
    "    sin = sin.unsqueeze(unsqueeze_dim)\n",
    "    \n",
    "    q_embedding = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embdedding = (q * cos ) + (rotate_half(k) * sin)\n",
    "\n",
    "    return q_embedding, k_embdedding\n",
    "\n",
    "class RotaryEmbedding(nn.Module):\n",
    "    def __init__(self, dim: int, max_position_embedding: int=1024, base:int = 1_0000, device: torch.device=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.hidden_size = dim\n",
    "        self.max_position_embedding = max_position_embedding\n",
    "        self.base = base\n",
    "\n",
    "        inv_freq = 1.0 / (self.base ** (\n",
    "            torch.arange(0, dim, 2, dtype=torch.int64).float().to(device=device) / dim\n",
    "        ))\n",
    "\n",
    "        self.register_buffer('inv_freq', inv_freq, persistent=False)\n",
    "        self.max_seq_len_cached = None\n",
    "    \n",
    "    def _set_cos_sin_cache(self, seq_len: int, device: torch.device, dtype: torch.dtype):\n",
    "        self.max_seq_len_cached = seq_len\n",
    "\n",
    "        t = torch.arange(self.max_seq_len_cached, device=device, dtype=self.inv_freq.dtype)\n",
    "\n",
    "        new_freqs = torch.outer(t, self.inv_freq.to(device=t.device))\n",
    "        embedding = torch.cat((new_freqs, new_freqs), dim=-1)\n",
    "        self.register_buffer('cos_cached', embedding.sin().to(dtype=dtype), persistent=False)\n",
    "        self.register_buffer('sin_cached', embedding.cos().to(dtype=dtype), persistent=False)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def forward(self, x: Tensor, seq_len: Tensor|int):\n",
    "        if self.max_seq_len_cached is None or seq_len > self.max_seq_len_cached:\n",
    "            self._set_cos_sin_cache(seq_len=seq_len, device=x.device, dtype=x.dtype)\n",
    "\n",
    "        return (\n",
    "            self.cos_cached[: seq_len].to(dtype=x.dtype),\n",
    "            self.sin_cached[: seq_len].to(dtype=x.dtype),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class MLAConfig:\n",
    "    hidden_size: int = 1024\n",
    "    num_heads: int = 8\n",
    "    assert hidden_size % num_heads == 0\n",
    "\n",
    "    max_position_embeddings: int = 1024\n",
    "    rope_theta: float = 10_0000.0\n",
    "    attention_dropout: float = 0.1\n",
    "\n",
    "    q_lora_rank: int = 128\n",
    "    \n",
    "    qk_rope_head_dim: int = 8\n",
    "    qk_nope_head_dim: int = 24\n",
    "    q_head_dim: int = qk_rope_head_dim + qk_nope_head_dim\n",
    "\n",
    "    assert  hidden_size % q_head_dim == 0\n",
    "\n",
    "    kv_lora_rank: int = 32\n",
    "    v_head_dim: int = 16\n",
    "    \n",
    "    assert hidden_size % v_head_dim == 0\n",
    "    attention_bias: bool=False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1000, 1024])\n",
      "torch.Size([2, 8, 1000, 1000])\n"
     ]
    }
   ],
   "source": [
    "class MLA(nn.Module):\n",
    "    def __init__(self, config: MLAConfig):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.config = config\n",
    "        self.q_head_dim = config.qk_nope_head_dim + config.qk_rope_head_dim\n",
    "\n",
    "        # q\n",
    "        self.q_down_proj = nn.Linear(\n",
    "            in_features=config.hidden_size,\n",
    "            out_features=config.q_lora_rank,\n",
    "            bias=config.attention_bias,\n",
    "        )\n",
    "        self.q_down_layernorm = RMSNorm(config.q_lora_rank)\n",
    "\n",
    "        self.q_up_proj = nn.Linear(\n",
    "            in_features=config.q_lora_rank,\n",
    "            # out_features并不是hidden_size，最终需要拆分为需要应用rope和nope两部分\n",
    "            out_features=config.num_heads * self.q_head_dim, \n",
    "            bias=config.attention_bias,\n",
    "        )\n",
    "        self.q_up_layernorm = RMSNorm(config.q_lora_rank)\n",
    "\n",
    "\n",
    "        # kv\n",
    "        self.kv_down_proj = nn.Linear(\n",
    "            in_features=config.hidden_size,\n",
    "            out_features=config.kv_lora_rank + config.qk_rope_head_dim,\n",
    "            bias=False,\n",
    "        )\n",
    "\n",
    "        self.kv_down_layernorm = RMSNorm(config.kv_lora_rank)\n",
    "\n",
    "        self.kv_up_proj = nn.Linear(\n",
    "            in_features=config.kv_lora_rank,\n",
    "            out_features=config.num_heads * (config.qk_nope_head_dim + config.v_head_dim),\n",
    "            bias=False,\n",
    "        )\n",
    "\n",
    "        self.out_proj = nn.Linear(\n",
    "            in_features=config.num_heads * config.v_head_dim,\n",
    "            out_features=config.hidden_size,\n",
    "            bias=config.attention_bias,\n",
    "        )\n",
    "        \n",
    "        self.rotary_emb = RotaryEmbedding(\n",
    "            dim=config.qk_rope_head_dim,\n",
    "            max_position_embedding=config.max_position_embeddings,\n",
    "            base=config.rope_theta,\n",
    "        )\n",
    "\n",
    "        self.atten_factor = 1.0 / np.sqrt(config.qk_nope_head_dim)\n",
    "\n",
    "    def forward(self, \n",
    "                hidden_states: Tensor,\n",
    "                attention_mask: Tensor=None,\n",
    "                position_ids: Tensor=None,\n",
    "            ):\n",
    "        \n",
    "        bs, q_len, _ = hidden_states.shape\n",
    "        config = self.config\n",
    "\n",
    "        # 1. q\n",
    "        q: Tensor = self.q_down_proj(hidden_states)\n",
    "        q = self.q_down_layernorm(q) # [bs, sql_len, q_lora_rank]\n",
    "        q = self.q_up_proj(q) # [bs, sql_len, num_heads * (qk_nope_head_dim + qk_rope_head_dim)]\n",
    "        q = q.reshape(bs, q_len, config.num_heads, self.q_head_dim)\n",
    "\n",
    "        q_nope, q_pe = torch.split(\n",
    "            q,\n",
    "            [config.qk_nope_head_dim, config.qk_rope_head_dim],\n",
    "            dim=-1,\n",
    "        )\n",
    "\n",
    "        # 2. kv\n",
    "        downed_kv: Tensor = self.kv_down_proj(hidden_states)\n",
    "        downed_kv, k_pe = torch.split(\n",
    "            downed_kv,\n",
    "            [config.kv_lora_rank, config.qk_rope_head_dim],\n",
    "            dim=-1,\n",
    "        )\n",
    "        k_pe = k_pe.reshape(bs, q_len, 1, config.qk_rope_head_dim)\n",
    "\n",
    "        # [bs, q_len, kv_lora_rank]\n",
    "        kv: Tensor = self.kv_down_layernorm(downed_kv)\n",
    "\n",
    "        ## TODO kv cache here\n",
    "\n",
    "        kv = self.kv_up_proj(kv) # [bs, q_len, num_heads * (qk_nope_head_dim + v_head_dim)]\n",
    "        kv = kv.reshape(bs, q_len, config.num_heads, config.qk_nope_head_dim + config.v_head_dim)\n",
    "\n",
    "        k_nope, value_states = torch.split(\n",
    "            kv,\n",
    "            [config.qk_nope_head_dim, config.v_head_dim],\n",
    "            dim=-1,\n",
    "        )\n",
    "\n",
    "        # 3. q, k 应用旋转位置编码\n",
    "        cos, sin = self.rotary_emb(x=value_states, seq_len=q_len)\n",
    "        q_pe, k_pe = apply_rotay_pos_emb(q=q_pe, k=k_pe, cos=cos, sin=sin, position_ids=position_ids)\n",
    "\n",
    "        query_states = torch.empty(\n",
    "            bs, q_len, config.num_heads, self.q_head_dim,\n",
    "            device=k_pe.device,\n",
    "        )\n",
    "\n",
    "        query_states[:, :, :, : config.qk_nope_head_dim] = q_nope\n",
    "        query_states[:, :, :, config.qk_nope_head_dim: ] = q_pe\n",
    "\n",
    "        key_states = torch.empty(\n",
    "            bs, q_len, config.num_heads, self.q_head_dim,\n",
    "            device=k_pe.device,\n",
    "        )\n",
    "        key_states[:, :, :, : config.qk_nope_head_dim] = k_nope\n",
    "        key_states[:, :, :, config.qk_nope_head_dim: ] = k_pe\n",
    "\n",
    "        # 5. 注意力分数计算\n",
    "        query_states = query_states.transpose(1, 2)\n",
    "        key_states = key_states.transpose(1, 2)\n",
    "        value_states = value_states.transpose(1, 2)\n",
    "\n",
    "        atten_weights = torch.matmul(query_states, key_states.transpose(2, 3))\n",
    "        atten_weights *= self.atten_factor\n",
    "\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            atten_weights = torch.masked_fill(\n",
    "                atten_weights,\n",
    "                attention_mask == 0,\n",
    "                -torch.inf\n",
    "            )\n",
    "        \n",
    "            \n",
    "        # softmax\n",
    "        atten_weights = F.softmax(atten_weights, dim=-1, dtype=torch.float32).to(dtype=query_states.dtype)\n",
    "        atten_weights = F.dropout(atten_weights, p=config.attention_dropout, training=self.training)\n",
    "\n",
    "        # output\n",
    "        atten_output = torch.matmul(atten_weights, value_states)\n",
    "        atten_output = atten_output.reshape(bs, q_len, -1)\n",
    "\n",
    "        atten_output = self.out_proj(atten_output)\n",
    "\n",
    "        return atten_output, atten_weights\n",
    "\n",
    "\n",
    "def test():\n",
    "    config = MLAConfig()\n",
    "    model = MLA(config)\n",
    "\n",
    "    seq_len = 1000\n",
    "    batch_size = 2\n",
    "    x = torch.rand((batch_size, seq_len, config.hidden_size))\n",
    "    position_ids = torch.arange(seq_len).unsqueeze(0).expand(batch_size, -1)\n",
    "\n",
    "    atten_output, atten_weights = model(x, position_ids=position_ids)\n",
    "\n",
    "    print(atten_output.shape)\n",
    "    print(atten_weights.shape)\n",
    "\n",
    "test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
