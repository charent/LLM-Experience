{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "from datasets import load_dataset, load_from_disk\n",
    "import torch\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from trl import GRPOConfig, GRPOTrainer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下载数据\n",
    "\n",
    "# raw_dataset = load_dataset(r'swulling/gsm8k_chinese')\n",
    "# raw_dataset.save_to_disk('data/gsm8k_chinese')\n",
    "\n",
    "raw_dataset = load_from_disk('data/gsm8k_chinese')\n",
    "raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYS_PROMPT = '''\\\n",
    "你是一个输出思考过程的人工智能助手。按照以下格式输出：\n",
    "<think>\n",
    "在这里输出思考过程。\n",
    "</think>\n",
    "<answer>\n",
    "在这里输出最终答案。\n",
    "</answer>\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_map_to_conversational_format(item:  dict[str, list]):\n",
    "    '''\n",
    "    see: https://huggingface.co/docs/trl/v0.15.2/en/grpo_trainer#using-a-custom-reward-function\n",
    "    '''\n",
    "    prompt_list = []\n",
    "    responses_list = []\n",
    "    for question, answer in zip(item['question_zh-cn'], item['answer_only']):\n",
    "\n",
    "        prompt_list.append(\n",
    "            [{\"role\": \"system\", \"content\": SYS_PROMPT}, {\"role\": \"user\", \"content\": str(question)}]\n",
    "        )\n",
    "        responses_list.append(\n",
    "            [{\"role\": \"assistant\", \"content\": str(answer).strip()}]\n",
    "        )\n",
    "\n",
    "    return {\n",
    "        'prompt': prompt_list,\n",
    "        'response': responses_list,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maped_dataset = raw_dataset['train'].map(dataset_map_to_conversational_format, num_proc=2, batch_size=4, batched=True, remove_columns=raw_dataset['train'].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maped_dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 奖励函数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FORMAT_LABELS = ['<think>\\n', '</think>\\n', '<answer>\\n', '</answer>\\n']\n",
    "ANSWER_RE = re.compile(r'.*<answer>(.*?)</answer>.*',re.DOTALL)\n",
    "THINK_RE =  re.compile(r'.*<think>(.*?)</think>.*',re.DOTALL)\n",
    "NUM_RE = re.compile(r'^[+-]?\\d+\\.?\\d+?$')\n",
    "FORMAT_RE = re.compile(r\"^<think>\\n.*?\\n</think>\\n<answer>\\n.*?\\n</answer>\\n$\")\n",
    "\n",
    "def get_part_format_reward(text: str):\n",
    "    '''\n",
    "    获取部分格式正确的奖励\n",
    "    '''\n",
    "    label_counts = [text.count(label) for label in FORMAT_LABELS]\n",
    "    total_rewards = sum(0.25 if l_cnt == 1 else 0.0 for l_cnt in label_counts)\n",
    "    return float(total_rewards)\n",
    "\n",
    "def extract_completion_think_or_answer(text: str, pattern: re.Pattern) -> str:\n",
    "    '''\n",
    "    提取答案或思考过程\n",
    "    '''\n",
    "    matchs = pattern.findall(text)\n",
    "    if matchs:\n",
    "        return str(matchs[0].strip())\n",
    "    return ''\n",
    "\n",
    "# --------------------------------------------\n",
    "\n",
    "def part_format_reward_func(completions: list[dict], **kwargs):\n",
    "    '''\n",
    "    部分格式奖励\n",
    "    '''\n",
    "    completion_contents: list[str] = [completion[0][\"content\"] for completion in completions]\n",
    "    rewards = []\n",
    "    for content in completion_contents:\n",
    "        rw = get_part_format_reward(text=content)\n",
    "        if content.startswith('<think>'):\n",
    "            rw += 0.25\n",
    "        if content.strip().endswith('</answer>'):\n",
    "            rw += 0.25\n",
    "        rewards.append(rw)\n",
    "    return rewards\n",
    "\n",
    "def format_reward_func(completions: list[dict], **kwargs):\n",
    "    '''\n",
    "    回到格式完全正确奖励\n",
    "    '''\n",
    "    completion_contents = [completion[0][\"content\"] for completion in completions]\n",
    "    matches = [FORMAT_RE.match(content) for content in completion_contents]\n",
    "    return [1.5 if match else 0.0 for match in matches]\n",
    "\n",
    "def answer_digit_reward_func(prompts: list[dict], completions: list[dict], response: list[dict], **kwargs):\n",
    "    '''\n",
    "    答案是数字奖励\n",
    "    '''\n",
    "    completion_contents = [completion[0][\"content\"] for completion in completions]\n",
    "    labels_answers = [completion[0][\"content\"] for completion in response]\n",
    "    completion_answers = [extract_completion_think_or_answer(content, ANSWER_RE) for content in completion_contents]\n",
    "    rewards = []\n",
    "    for label, predict in zip(labels_answers, completion_answers):\n",
    "        if label == predict:\n",
    "           rewards.append(1.5)\n",
    "        elif label in predict and label != predict:\n",
    "            # 答案部分匹配\n",
    "            rewards.append(1.0)\n",
    "        elif NUM_RE.match(predict):\n",
    "            # 是数字就行\n",
    "            rewards.append(0.5)\n",
    "        else:\n",
    "            rewards.append(0.0)\n",
    "    return rewards\n",
    "\n",
    "def answer_correct_reward_func(completions: list[dict], response: list[dict], **kwargs):\n",
    "    '''\n",
    "    答案正确性奖励, completions 模型生成内容，response 数据集的标准答案\n",
    "    '''\n",
    "    completion_contents = [completion[0][\"content\"] for completion in completions]\n",
    "    labels_answers = [completion[0][\"content\"] for completion in response]\n",
    "    completion_answers = [extract_completion_think_or_answer(content, ANSWER_RE) for content in completion_contents]\n",
    "\n",
    "    rewards = [2.5 if label == predict else 0.0 for label, predict in zip(labels_answers, completion_answers)]\n",
    "\n",
    "    return rewards\n",
    "    \n",
    "def think_length_reward_func(completions: list[dict], **kwargs):\n",
    "    '''\n",
    "    思考长度奖励\n",
    "    '''\n",
    "    completion_contents = [completion[0][\"content\"] for completion in completions]\n",
    "    completion_thinks = [extract_completion_think_or_answer(content, THINK_RE) for content in completion_contents]\n",
    "    \n",
    "    rewards = []\n",
    "    for think in completion_thinks:\n",
    "        if len(think) <= 20:\n",
    "            rewards.append(0.2)\n",
    "        elif len(think) >= 500:\n",
    "            rewards.append(0.2)\n",
    "        elif len(think) >= 400:\n",
    "            rewards.append(0.5)\n",
    "        elif len(think) >= 300:\n",
    "            rewards.append(0.8)\n",
    "        else:\n",
    "            # 偏向短思考 300 字以下\n",
    "            rewards.append(1.5)\n",
    "\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = GRPOConfig(\n",
    "    output_dir=\"output_models/Qwen2-0.5B-GRPO\", \n",
    "    save_only_model=True,\n",
    "    save_steps=100,\n",
    "    warmup_ratio=0.01,\n",
    "    report_to='tensorboard',\n",
    "    warmup_steps=10,\n",
    "    bf16=True, \n",
    "    learning_rate=5e-5,\n",
    "    logging_steps=1, \n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=1,\n",
    "    num_generations=8,\n",
    "    max_prompt_length=384,\n",
    "    max_completion_length=384,\n",
    "    use_vllm=False,\n",
    "    torch_empty_cache_steps=1,\n",
    "    lr_scheduler_type='constant_with_warmup',\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_lora = False\n",
    "model_path = r'/mnt/sdc/models_home/Qwen2___5-0___5B-Instruct'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "if use_lora:\n",
    "    lora_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        r=16,\n",
    "        lora_alpha=32,\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",]\n",
    "    )\n",
    "    model = get_peft_model(model, peft_config=lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = GRPOTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    reward_funcs=[\n",
    "        part_format_reward_func,\n",
    "        format_reward_func,\n",
    "        answer_digit_reward_func,\n",
    "        answer_correct_reward_func,\n",
    "        think_length_reward_func,\n",
    "    ],\n",
    "    args=training_args,\n",
    "    train_dataset=maped_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
